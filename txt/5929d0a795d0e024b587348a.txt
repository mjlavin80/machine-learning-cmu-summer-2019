A troubling feature of political disagreement in the United States today is that many issues on which liberals and conservatives hold divergent views are questions not of value but of fact. Is human activity responsible for global warming? Do guns make society safer? Is immigration harmful to the economy? Though undoubtedly complicated, these questions turn on empirical evidence. As new information emerges, we ought to move, however fitfully, toward consensus. But we don’t. Unfortunately, people do not always revise their beliefs in light of new information. On the contrary, they often stubbornly maintain their views. Certain disagreements stay entrenched and polarized. Why? A common explanation is confirmation bias. This is the psychological tendency to favor information that confirms our beliefs and to disfavor information that counters them — a tendency manifested in the echo chambers and “filter bubbles” of the online world. If this explanation is right, then there is a relatively straightforward solution to political polarization: We need to consciously expose ourselves to evidence that challenges our beliefs to compensate for our inclination to discount it. But what if confirmation bias isn’t the only culprit? It recently struck us that confirmation bias is often conflated with “telling people what they want to hear,” which is actually a distinct phenomenon known as desirability bias, or the tendency to credit information you want to believe. Though there is a clear difference between what you believe and what you want to believe — a pessimist may expect the worst but hope for the best — when it comes to political beliefs, they are frequently aligned. For example, gun-control advocates who believe stricter firearms laws will reduce gun-related homicides usually also want to believe that such laws will reduce gun-related homicides. If those advocates decline to revise their beliefs in the face of evidence to the contrary, it can be hard to tell which bias is at work. So we decided to conduct an experiment that would isolate these biases. This way, we could see whether a reluctance to revise political beliefs was a result of confirmation bias or desirability bias (or both). Our experiment capitalized on the fact that one month before the 2016 presidential election there was a profusion of close polling results concerning Donald Trump and Hillary Clinton. We asked 900 United States residents which candidate they wanted to win the election, and which candidate they believed was most likely to win. Respondents fell into two groups. In one group were those who believed the candidate they wanted to win was also most likely to win (for example, the Clinton supporter who believed Mrs. Clinton would win). In the other group were those who believed the candidate they wanted to win was not the candidate most likely to win (for example, the Trump supporter who believed Mrs. Clinton would win). Each person in the study then read about recent polling results emphasizing either that Mrs. Clinton or Mr. Trump was more likely to win. Roughly half of our participants believed their preferred candidate was the one less likely to win the election. For those people, the desirability of the polling evidence was decoupled from its value in confirming their beliefs. After reading about the recent polling numbers, all the participants once again indicated which candidate they believed was most likely to win. The results, which we report in a forthcoming paper in the Journal of Experimental Psychology: General, were clear and robust. Those people who received desirable evidence — polls suggesting that their preferred candidate was going to win — took note and incorporated the information into their subsequent belief about which candidate was most likely to win the election. In contrast, those people who received undesirable evidence barely changed their belief about which candidate was most likely to win. Importantly, this bias in favor of the desirable evidence emerged irrespective of whether the polls confirmed or disconfirmed peoples’ prior belief about which candidate would win. In other words, we observed a general bias toward the desirable evidence. What about confirmation bias? To our surprise, those people who received confirming evidence — polls supporting their prior belief about which candidate was most likely to win — showed no bias in favor of this information. They tended to incorporate this evidence into their subsequent belief to the same extent as those people who had their prior belief disconfirmed. In other words, we observed little to no bias toward the confirming evidence. We also explored which supporters showed the greatest bias in favor of the desirable evidence. The results were bipartisan: Supporters of Mr. Trump and supporters of Mrs. Clinton showed a similar-size bias in favor of the desirable evidence. Our study suggests that political belief polarization may emerge because of peoples’ conflicting desires, not their conflicting beliefs per se. This is rather troubling, as it implies that even if we were to escape from our political echo chambers, it wouldn’t help much. Short of changing what people want to believe, we must find other ways to unify our perceptions of reality.
The spread of misinformation on social media is an alarming phenomenon that scientists have yet to fully understand. While the data show that false claims are increasing online, most studies have analyzed only small samples or the spread of individual fake stories. My colleagues Soroush Vosoughi, Deb Roy and I set out to change that. We recently analyzed the diffusion of all of the major true and false stories that spread on Twitter from its inception in 2006 to 2017. Our data included approximately 126,000 Twitter “cascades” (unbroken chains of retweets with a common, singular origin) involving stories spread by three million people more than four and a half million times. Disturbingly, we found that false stories spread significantly more than did true ones. Our findings were published on Thursday in the journal Science. We started by identifying thousands of true and false stories, using information from six independent fact-checking organizations, including Snopes, PolitiFact and Factcheck.org. These organizations exhibited considerable agreement — between 95 percent and 98 percent — on the truth or falsity of these stories. Then we searched Twitter for mentions of these stories, followed the sharing activity to the “origin” tweets (the first mention of a story on Twitter) and traced all the retweet cascades from every origin tweet. We then analyzed how they spread online. For all categories of information — politics, entertainment, business and so on — we found that false stories spread significantly farther, faster and more broadly than did true ones. Falsehoods were 70 percent more likely to be retweeted, even when controlling for the age of the original tweeter’s account, its activity level, the number of its followers and followees, and whether Twitter had verified the account as genuine. These effects were more pronounced for false political stories than for any other type of false news. Surprisingly, Twitter users who spread false stories had, on average, significantly fewer followers, followed significantly fewer people, were significantly less active on Twitter, were verified as genuine by Twitter significantly less often and had been on Twitter for significantly less time than were Twitter users who spread true stories. Falsehood diffused farther and faster despite these seeming shortcomings. And despite concerns about the role of web robots in spreading false stories, we found that human behavior contributed more to the differential spread of truth and falsity than bots did. Using established bot-detection algorithms, we found that bots accelerated the spread of true stories at approximately the same rate as they accelerated the spread of false stories, implying that false stories spread more than true ones as a result of human activity. Why would that be? One explanation is novelty. Perhaps the novelty of false stories attracts human attention and encourages sharing, conveying status on sharers who seem more “in the know.” Our analysis seemed to bear out this hypothesis. Using accepted computerized methods for inferring emotional content from word use, we found that false stories inspired replies on Twitter expressing greater surprise than did true stories. The truth, on the other hand, inspired more joy and trust. Such emotions may shed light on what inspires people to share false stories. As we learn more about how and why false news spreads, we should test interventions to dampen its diffusion. For example, though it was disheartening to learn that humans are more responsible for the spread of false stories than previously thought, this finding also implies that behavioral interventions may succeed in stemming the tide of falsity. It could be, for example, that labeling news stories, in much the same way we label food, could change the way people consume and share it. Financial incentives are another possible tool. The social media advertising market creates incentives for the spread of false stories because their wider diffusion makes them profitable. If platforms were to demote accounts or posts that disseminated false stories, using algorithms to weed out falsehoods, the financial incentives would presumably be reduced. The tricky question, of course, would be: Who gets to decide what is true and false? Our research is just the beginning. A more robust identification of the factors that drive the spread of true and false news will require direct interaction with users through interviews, surveys and lab experiments. We could also benefit from randomized controlled trials of efforts to dampen the spread of false stories. Some notion of truth is central to the proper functioning of nearly every realm of human endeavor. If we allow the world to be consumed by falsity, we are inviting catastrophe.